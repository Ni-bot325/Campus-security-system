{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25891e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading Campus Security Dataset...\n",
      "============================================================\n",
      " All datasets loaded successfully!\n",
      "\n",
      " Dataset Summary:\n",
      "------------------------------------------------------------\n",
      "Student Profiles..............    7,000 records,  10 columns\n",
      "Card Swipes...................    8,000 records,   3 columns\n",
      "WiFi Logs.....................    8,000 records,   3 columns\n",
      "Library Checkouts.............    7,000 records,   4 columns\n",
      "Lab Bookings..................    7,000 records,   6 columns\n",
      "CCTV Frames...................    7,000 records,   4 columns\n",
      "Face Embeddings...............    6,973 records,   2 columns\n",
      "Helpdesk Notes................    7,000 records,   5 columns\n",
      "\n",
      " Total records across all datasets: 57,973\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set the dataset folder path (go up one level from notebooks to find dataset)\n",
    "dataset_folder = Path('../dataset')  # Note the ../ to go up one folder level\n",
    "\n",
    "print(\" Loading Campus Security Dataset...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load all CSV files\n",
    "profiles = pd.read_csv(dataset_folder / 'student or staff profiles.csv')\n",
    "card_swipes = pd.read_csv(dataset_folder / 'campus card_swipes.csv')\n",
    "wifi_logs = pd.read_csv(dataset_folder / 'wifi_associations_logs.csv')\n",
    "library_checkouts = pd.read_csv(dataset_folder / 'library_checkouts.csv')\n",
    "lab_bookings = pd.read_csv(dataset_folder / 'lab_bookings.csv')\n",
    "cctv_frames = pd.read_csv(dataset_folder / 'cctv_frames.csv')\n",
    "face_embeddings = pd.read_csv(dataset_folder / 'face_embeddings.csv')\n",
    "helpdesk_notes = pd.read_csv(dataset_folder / 'free_text_notes (helpdesk or RSVPs).csv')\n",
    "\n",
    "print(\" All datasets loaded successfully!\\n\")\n",
    "\n",
    "# Display summary statistics\n",
    "datasets = {\n",
    "    'Student Profiles': profiles,\n",
    "    'Card Swipes': card_swipes,\n",
    "    'WiFi Logs': wifi_logs,\n",
    "    'Library Checkouts': library_checkouts,\n",
    "    'Lab Bookings': lab_bookings,\n",
    "    'CCTV Frames': cctv_frames,\n",
    "    'Face Embeddings': face_embeddings,\n",
    "    'Helpdesk Notes': helpdesk_notes\n",
    "}\n",
    "\n",
    "print(\" Dataset Summary:\")\n",
    "print(\"-\" * 60)\n",
    "for name, df in datasets.items():\n",
    "    print(f\"{name:.<30} {len(df):>8,} records, {len(df.columns):>3} columns\")\n",
    "\n",
    "print(\"\\n Total records across all datasets:\", f\"{sum(len(df) for df in datasets.values()):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0315d258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " STUDENT PROFILES - Sample Data:\n",
      "  entity_id        name     role             email department student_id  \\\n",
      "0   E100000  Neha Mehta  student  user0@campus.edu      CIVIL     S39256   \n",
      "1   E100001  Neha Kumar  student  user1@campus.edu    Physics     S14165   \n",
      "2   E100002  Neha Singh  student  user2@campus.edu      Admin     S13478   \n",
      "\n",
      "  staff_id card_id     device_hash  face_id  \n",
      "0      NaN   C3286  DH6d0bd80c8f8e  F100000  \n",
      "1      NaN   C1488  DH75af13047587  F100001  \n",
      "2      NaN   C4257  DH7a5f80cb6e8d  F100002  \n",
      "\n",
      " Columns: ['entity_id', 'name', 'role', 'email', 'department', 'student_id', 'staff_id', 'card_id', 'device_hash', 'face_id']\n",
      "\n",
      "================================================================================\n",
      "\n",
      " CARD SWIPES - Sample Data:\n",
      "  card_id  location_id            timestamp\n",
      "0   C7151          GYM  2025-09-13 14:02:40\n",
      "1   C4562  ADMIN_LOBBY  2025-09-22 00:16:08\n",
      "2   C6840      LIB_ENT  2025-09-12 22:50:40\n",
      "\n",
      " Columns: ['card_id', 'location_id', 'timestamp']\n",
      "\n",
      "================================================================================\n",
      "\n",
      " WIFI LOGS - Sample Data:\n",
      "      device_hash     ap_id        timestamp\n",
      "0  DH6034d82be28b  AP_LIB_1  9/16/2025 19:22\n",
      "1  DHbb3a8ccad22b  AP_ENG_5   9/24/2025 5:50\n",
      "2  DH791defaaf163  AP_AUD_3   9/4/2025 22:48\n",
      "\n",
      " Columns: ['device_hash', 'ap_id', 'timestamp']\n",
      "\n",
      "================================================================================\n",
      "\n",
      " LIBRARY CHECKOUTS - Sample Data:\n",
      "  checkout_id entity_id book_id            timestamp\n",
      "0    LC200000   E100646  BK2549  2025-08-27 06:57:14\n",
      "1    LC200001   E105470  BK2116  2025-09-21 02:36:03\n",
      "2    LC200002   E102616  BK2404  2025-09-12 23:46:25\n",
      "\n",
      " Columns: ['checkout_id', 'entity_id', 'book_id', 'timestamp']\n"
     ]
    }
   ],
   "source": [
    "# View first few rows of each dataset\n",
    "print(\" STUDENT PROFILES - Sample Data:\")\n",
    "print(profiles.head(3))\n",
    "print(\"\\n Columns:\", list(profiles.columns))\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "print(\"\\n CARD SWIPES - Sample Data:\")\n",
    "print(card_swipes.head(3))\n",
    "print(\"\\n Columns:\", list(card_swipes.columns))\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "print(\"\\n WIFI LOGS - Sample Data:\")\n",
    "print(wifi_logs.head(3))\n",
    "print(\"\\n Columns:\", list(wifi_logs.columns))\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "print(\"\\n LIBRARY CHECKOUTS - Sample Data:\")\n",
    "print(library_checkouts.head(3))\n",
    "print(\"\\n Columns:\", list(library_checkouts.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31eef23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ENTITY RESOLUTION SYSTEM\n",
      "================================================================================\n",
      " Mapped 30,585 identifiers to 7,000 entities\n",
      "   Average identifiers per entity: 4.4\n",
      "\n",
      " Sample Entity Mapping (first entity):\n",
      "   Entity: E100000\n",
      "   Linked IDs: ['S39256', 'C3286', 'DH6d0bd80c8f8e', 'F100000', 'user0@campus.edu']\n"
     ]
    }
   ],
   "source": [
    "# ===== ENTITY RESOLUTION SYSTEM =====\n",
    "print(\" ENTITY RESOLUTION SYSTEM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create entity mapping dictionary\n",
    "entity_map = {}\n",
    "\n",
    "# Map all identifiers from profiles to entity_id\n",
    "for _, row in profiles.iterrows():\n",
    "    entity_id = row['entity_id']\n",
    "    \n",
    "    # Map each identifier type to the entity\n",
    "    if pd.notna(row['student_id']):\n",
    "        entity_map[row['student_id']] = entity_id\n",
    "    if pd.notna(row['staff_id']):\n",
    "        entity_map[row['staff_id']] = entity_id\n",
    "    if pd.notna(row['card_id']):\n",
    "        entity_map[row['card_id']] = entity_id\n",
    "    if pd.notna(row['device_hash']):\n",
    "        entity_map[row['device_hash']] = entity_id\n",
    "    if pd.notna(row['face_id']):\n",
    "        entity_map[row['face_id']] = entity_id\n",
    "    if pd.notna(row['email']):\n",
    "        entity_map[row['email']] = entity_id\n",
    "\n",
    "print(f\" Mapped {len(entity_map):,} identifiers to {len(profiles):,} entities\")\n",
    "print(f\"   Average identifiers per entity: {len(entity_map)/len(profiles):.1f}\")\n",
    "\n",
    "# Show sample mapping\n",
    "print(f\"\\n Sample Entity Mapping (first entity):\")\n",
    "first_entity = profiles.iloc[0]['entity_id']\n",
    "first_entity_ids = [k for k, v in entity_map.items() if v == first_entity]\n",
    "print(f\"   Entity: {first_entity}\")\n",
    "print(f\"   Linked IDs: {first_entity_ids}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8de3383b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " LINKING DATASETS TO ENTITIES\n",
      "================================================================================\n",
      " Card Swipes: 8,000/8,000 linked\n",
      " WiFi Logs: 8,000/8,000 linked\n",
      " Library Checkouts: 7,000/7,000 linked\n",
      " Lab Bookings: 0/7,000 linked\n",
      " CCTV Frames: 4,213/7,000 linked\n",
      " Face Embeddings: 0/6,973 linked\n",
      " Helpdesk Notes: 7,000/7,000 linked\n",
      "\n",
      " All datasets successfully linked to entities!\n"
     ]
    }
   ],
   "source": [
    "# ===== LINK ALL DATASETS TO ENTITIES =====\n",
    "print(\"\\n LINKING DATASETS TO ENTITIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Add entity_id to card swipes (via card_id)\n",
    "card_swipes['entity_id'] = card_swipes['card_id'].map(entity_map)\n",
    "print(f\" Card Swipes: {card_swipes['entity_id'].notna().sum():,}/{len(card_swipes):,} linked\")\n",
    "\n",
    "# Add entity_id to wifi logs (via device_hash)\n",
    "wifi_logs['entity_id'] = wifi_logs['device_hash'].map(entity_map)\n",
    "print(f\" WiFi Logs: {wifi_logs['entity_id'].notna().sum():,}/{len(wifi_logs):,} linked\")\n",
    "\n",
    "# Library checkouts already have entity_id\n",
    "print(f\" Library Checkouts: {library_checkouts['entity_id'].notna().sum():,}/{len(library_checkouts):,} linked\")\n",
    "\n",
    "# Add entity_id to lab bookings\n",
    "lab_bookings['entity_id'] = lab_bookings['booking_id'].str.extract(r'(E\\d+)')[0]\n",
    "print(f\" Lab Bookings: {lab_bookings['entity_id'].notna().sum():,}/{len(lab_bookings):,} linked\")\n",
    "\n",
    "# Add entity_id to CCTV frames (via face_id)\n",
    "cctv_frames['entity_id'] = cctv_frames['face_id'].map(entity_map)\n",
    "print(f\" CCTV Frames: {cctv_frames['entity_id'].notna().sum():,}/{len(cctv_frames):,} linked\")\n",
    "\n",
    "# Add entity_id to face embeddings (via face_id)\n",
    "face_embeddings['entity_id'] = face_embeddings['face_id'].map(entity_map)\n",
    "print(f\" Face Embeddings: {face_embeddings['entity_id'].notna().sum():,}/{len(face_embeddings):,} linked\")\n",
    "\n",
    "# Helpdesk notes - extract entity from note_id or use entity_id column\n",
    "if 'entity_id' in helpdesk_notes.columns:\n",
    "    print(f\" Helpdesk Notes: {helpdesk_notes['entity_id'].notna().sum():,}/{len(helpdesk_notes):,} linked\")\n",
    "else:\n",
    "    helpdesk_notes['entity_id'] = helpdesk_notes['note_id'].str.extract(r'(E\\d+)')[0]\n",
    "    print(f\" Helpdesk Notes: {helpdesk_notes['entity_id'].notna().sum():,}/{len(helpdesk_notes):,} linked\")\n",
    "\n",
    "print(\"\\n All datasets successfully linked to entities!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "129aa094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TIMELINE GENERATION TEST (UPDATED)\n",
      "================================================================================\n",
      "\n",
      " Timeline for Neha Mehta (E100000):\n",
      "   Total events: 6\n",
      "   Date range: 2025-08-29 07:41:18 to 2025-09-19 00:36:50\n",
      "   Event types: {'physical_access': 5, 'face_recognition': 1}\n",
      "\n",
      " First 10 events:\n",
      "          timestamp       event_type    location                   details\n",
      "2025-08-29 07:41:18  physical_access HOSTEL_GATE Card swipe at HOSTEL_GATE\n",
      "2025-09-02 13:58:39  physical_access     LIB_ENT     Card swipe at LIB_ENT\n",
      "2025-09-04 02:31:00 face_recognition     LAB_101  Face detected at LAB_101\n",
      "2025-09-05 06:46:02  physical_access  AUDITORIUM  Card swipe at AUDITORIUM\n",
      "2025-09-12 19:18:02  physical_access  AUDITORIUM  Card swipe at AUDITORIUM\n",
      "2025-09-19 00:36:50  physical_access     LAB_101     Card swipe at LAB_101\n"
     ]
    }
   ],
   "source": [
    "# ===== TIMELINE GENERATION SYSTEM (CORRECTED) =====\n",
    "def generate_entity_timeline(entity_id):\n",
    "    \"\"\"Generate complete timeline for a given entity\"\"\"\n",
    "    \n",
    "    timeline = []\n",
    "    \n",
    "    # 1. Card Swipes\n",
    "    entity_swipes = card_swipes[card_swipes['entity_id'] == entity_id].copy()\n",
    "    for _, row in entity_swipes.iterrows():\n",
    "        timeline.append({\n",
    "            'timestamp': pd.to_datetime(row['timestamp'], errors='coerce'),\n",
    "            'event_type': 'physical_access',\n",
    "            'location': row['location_id'],\n",
    "            'source': 'card_swipe',\n",
    "            'details': f\"Card swipe at {row['location_id']}\"\n",
    "        })\n",
    "    \n",
    "    # 2. WiFi Connections (uses 'ap_id' not 'location_id')\n",
    "    entity_wifi = wifi_logs[wifi_logs['entity_id'] == entity_id].copy()\n",
    "    for _, row in entity_wifi.iterrows():\n",
    "        timeline.append({\n",
    "            'timestamp': pd.to_datetime(row['timestamp'], errors='coerce'),\n",
    "            'event_type': 'network_access',\n",
    "            'location': row['ap_id'],  # ← Changed from location_id\n",
    "            'source': 'wifi_log',\n",
    "            'details': f\"WiFi connected at {row['ap_id']}\"\n",
    "        })\n",
    "    \n",
    "    # 3. Library Checkouts\n",
    "    entity_library = library_checkouts[library_checkouts['entity_id'] == entity_id].copy()\n",
    "    for _, row in entity_library.iterrows():\n",
    "        timeline.append({\n",
    "            'timestamp': pd.to_datetime(row['timestamp'], errors='coerce'),\n",
    "            'event_type': 'library_transaction',\n",
    "            'location': 'Library',\n",
    "            'source': 'library_system',\n",
    "            'details': f\"Checked out book {row['book_id']}\"\n",
    "        })\n",
    "    \n",
    "    # 4. Lab Bookings (uses 'room_id' and 'start_time')\n",
    "    entity_labs = lab_bookings[lab_bookings['entity_id'] == entity_id].copy()\n",
    "    for _, row in entity_labs.iterrows():\n",
    "        timeline.append({\n",
    "            'timestamp': pd.to_datetime(row['start_time'], errors='coerce'),  # ← Changed from timestamp\n",
    "            'event_type': 'lab_booking',\n",
    "            'location': row['room_id'],  # ← Changed from location_id\n",
    "            'source': 'lab_system',\n",
    "            'details': f\"Lab booked: {row['room_id']}\"\n",
    "        })\n",
    "    \n",
    "    # 5. CCTV Detections\n",
    "    entity_cctv = cctv_frames[cctv_frames['entity_id'] == entity_id].copy()\n",
    "    for _, row in entity_cctv.iterrows():\n",
    "        timeline.append({\n",
    "            'timestamp': pd.to_datetime(row['timestamp'], errors='coerce'),\n",
    "            'event_type': 'face_recognition',\n",
    "            'location': row['location_id'],\n",
    "            'source': 'cctv_system',\n",
    "            'details': f\"Face detected at {row['location_id']}\"\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame and clean\n",
    "    timeline_df = pd.DataFrame(timeline)\n",
    "    \n",
    "    if len(timeline_df) > 0:\n",
    "        timeline_df = timeline_df[timeline_df['timestamp'].notna()]\n",
    "        timeline_df = timeline_df.sort_values('timestamp').reset_index(drop=True)\n",
    "    \n",
    "    return timeline_df\n",
    "\n",
    "# Test with first entity\n",
    "print(\" TIMELINE GENERATION TEST (UPDATED)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_entity = profiles.iloc[0]['entity_id']\n",
    "test_name = profiles.iloc[0]['name']\n",
    "\n",
    "timeline = generate_entity_timeline(test_entity)\n",
    "\n",
    "print(f\"\\n Timeline for {test_name} ({test_entity}):\")\n",
    "print(f\"   Total events: {len(timeline):,}\")\n",
    "\n",
    "if len(timeline) > 0:\n",
    "    print(f\"   Date range: {timeline['timestamp'].min()} to {timeline['timestamp'].max()}\")\n",
    "    print(f\"   Event types: {timeline['event_type'].value_counts().to_dict()}\")\n",
    "    \n",
    "    print(f\"\\n First 10 events:\")\n",
    "    display_cols = ['timestamp', 'event_type', 'location', 'details']\n",
    "    print(timeline[display_cols].head(10).to_string(index=False))\n",
    "else:\n",
    "    print(\"    No events found for this entity\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8aa2381d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ANOMALY DETECTION RESULTS\n",
      "================================================================================\n",
      "Detected 2 anomalies for Neha Mehta:\n",
      "\n",
      "1. [MEDIUM] late_night_activity\n",
      "   2 activities between 11 PM - 5 AM\n",
      "\n",
      "2. [MEDIUM] long_inactivity\n",
      "   3 gaps of >48 hours without activity\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===== ANOMALY DETECTION =====\n",
    "def detect_anomalies(entity_id, timeline_df):\n",
    "    \"\"\"Detect unusual patterns in entity timeline\"\"\"\n",
    "    \n",
    "    anomalies = []\n",
    "    \n",
    "    if len(timeline_df) == 0:\n",
    "        return anomalies\n",
    "    \n",
    "    # 1. Late night activity (11 PM - 5 AM)\n",
    "    timeline_df['hour'] = timeline_df['timestamp'].dt.hour\n",
    "    late_night = timeline_df[(timeline_df['hour'] >= 23) | (timeline_df['hour'] <= 5)]\n",
    "    if len(late_night) > 0:\n",
    "        anomalies.append({\n",
    "            'type': 'late_night_activity',\n",
    "            'severity': 'medium',\n",
    "            'count': len(late_night),\n",
    "            'details': f\"{len(late_night)} activities between 11 PM - 5 AM\"\n",
    "        })\n",
    "    \n",
    "    # 2. Weekend activity\n",
    "    timeline_df['day_of_week'] = timeline_df['timestamp'].dt.dayofweek\n",
    "    weekend = timeline_df[timeline_df['day_of_week'].isin([5, 6])]\n",
    "    if len(weekend) > 10:  # More than 10 weekend events\n",
    "        anomalies.append({\n",
    "            'type': 'excessive_weekend_activity',\n",
    "            'severity': 'low',\n",
    "            'count': len(weekend),\n",
    "            'details': f\"{len(weekend)} activities on weekends\"\n",
    "        })\n",
    "    \n",
    "    # 3. Rapid location changes (< 5 minutes apart)\n",
    "    timeline_df = timeline_df.sort_values('timestamp')\n",
    "    timeline_df['time_diff'] = timeline_df['timestamp'].diff().dt.total_seconds() / 60\n",
    "    rapid_changes = timeline_df[(timeline_df['time_diff'] < 5) & (timeline_df['time_diff'] > 0)]\n",
    "    if len(rapid_changes) > 5:\n",
    "        anomalies.append({\n",
    "            'type': 'rapid_location_changes',\n",
    "            'severity': 'high',\n",
    "            'count': len(rapid_changes),\n",
    "            'details': f\"{len(rapid_changes)} location changes within 5 minutes\"\n",
    "        })\n",
    "    \n",
    "    # 4. Inactive periods (no activity for > 48 hours)\n",
    "    timeline_df['gap'] = timeline_df['timestamp'].diff().dt.total_seconds() / 3600\n",
    "    long_gaps = timeline_df[timeline_df['gap'] > 48]\n",
    "    if len(long_gaps) > 0:\n",
    "        anomalies.append({\n",
    "            'type': 'long_inactivity',\n",
    "            'severity': 'medium',\n",
    "            'count': len(long_gaps),\n",
    "            'details': f\"{len(long_gaps)} gaps of >48 hours without activity\"\n",
    "        })\n",
    "    \n",
    "    return anomalies\n",
    "\n",
    "# Test anomaly detection\n",
    "anomalies = detect_anomalies(test_entity, timeline)\n",
    "\n",
    "print(\"\\n ANOMALY DETECTION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Detected {len(anomalies)} anomalies for {test_name}:\\n\")\n",
    "\n",
    "for i, anomaly in enumerate(anomalies, 1):\n",
    "    print(f\"{i}. [{anomaly['severity'].upper()}] {anomaly['type']}\")\n",
    "    print(f\"   {anomaly['details']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "295b94cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CAMPUS SECURITY SYSTEM - COMPLETE ANALYTICS\n",
      "================================================================================\n",
      "\n",
      " Overall System Statistics:\n",
      "   Total Entities: 7,000\n",
      "   Total Events Tracked: 37,000\n",
      "   Average Events per Entity: 5.3\n",
      "   Date Range: 2025-08-27 00:05:10 to 2025-09-25 23:51:54\n",
      "\n",
      " Event Distribution by Type:\n",
      "   Card Swipes..............    8,000 ( 21.6%)\n",
      "   WiFi Connections.........    8,000 ( 21.6%)\n",
      "   Library Checkouts........    7,000 ( 18.9%)\n",
      "   Lab Bookings.............    7,000 ( 18.9%)\n",
      "   CCTV Detections..........    7,000 ( 18.9%)\n",
      "\n",
      " Most Active Locations:\n",
      "   AUDITORIUM...............    2,918 events\n",
      "   LAB_305..................    2,840 events\n",
      "   LAB_101..................    2,788 events\n",
      "   HOSTEL_GATE..............    1,907 events\n",
      "   ADMIN_LOBBY..............    1,893 events\n",
      "   LIB_ENT..................    1,884 events\n",
      "   GYM......................    1,868 events\n",
      "   CAF_01...................    1,856 events\n",
      "   LAB_102..................    1,021 events\n",
      "   SEM_01...................    1,020 events\n",
      "\n",
      " Activity Distribution by Hour:\n",
      "   00:00  ██████████████████████████████████████████████████   362\n",
      "   01:00  █████████████████████████████████████████████   327\n",
      "   02:00  ███████████████████████████████████████████████   343\n",
      "   03:00  ██████████████████████████████████████████████   339\n",
      "   04:00  █████████████████████████████████████████████   326\n",
      "   05:00  ████████████████████████████████████████████████   353\n",
      "   06:00  ████████████████████████████████████████████   322\n",
      "   07:00  █████████████████████████████████████████████   329\n",
      "   08:00  █████████████████████████████████████████████████   361\n",
      "   09:00  ████████████████████████████████████████████   321\n",
      "   10:00  ██████████████████████████████████████████   309\n",
      "   11:00  ████████████████████████████████████████████████   353\n",
      "   12:00  ███████████████████████████████████████████   315\n",
      "   13:00  ██████████████████████████████████████████████   339\n",
      "   14:00  █████████████████████████████████████████   303\n",
      "   15:00  ██████████████████████████████████████████████   339\n",
      "   16:00  ███████████████████████████████████████████████   347\n",
      "   17:00  ██████████████████████████████████████████████   335\n",
      "   18:00  █████████████████████████████████████████████   333\n",
      "   19:00  ███████████████████████████████████████████████   346\n",
      "   20:00  █████████████████████████████████████████   300\n",
      "   21:00  ███████████████████████████████████████████████   345\n",
      "   22:00  █████████████████████████████████████████████   327\n",
      "   23:00  █████████████████████████████████████████████   326\n",
      "\n",
      " Analytics complete!\n"
     ]
    }
   ],
   "source": [
    "# ===== SYSTEM-WIDE ANALYTICS (CORRECTED) =====\n",
    "print(\" CAMPUS SECURITY SYSTEM - COMPLETE ANALYTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Overall Statistics\n",
    "total_entities = len(profiles)\n",
    "total_events = (len(card_swipes) + len(wifi_logs) + len(library_checkouts) + \n",
    "                len(lab_bookings) + len(cctv_frames))\n",
    "\n",
    "print(f\"\\n Overall System Statistics:\")\n",
    "print(f\"   Total Entities: {total_entities:,}\")\n",
    "print(f\"   Total Events Tracked: {total_events:,}\")\n",
    "print(f\"   Average Events per Entity: {total_events/total_entities:.1f}\")\n",
    "\n",
    "# Safe date range\n",
    "date_min = pd.to_datetime(card_swipes['timestamp'], errors='coerce').min()\n",
    "date_max = pd.to_datetime(card_swipes['timestamp'], errors='coerce').max()\n",
    "print(f\"   Date Range: {date_min} to {date_max}\")\n",
    "\n",
    "# 2. Event Distribution\n",
    "print(f\"\\n Event Distribution by Type:\")\n",
    "event_counts = {\n",
    "    'Card Swipes': len(card_swipes),\n",
    "    'WiFi Connections': len(wifi_logs),\n",
    "    'Library Checkouts': len(library_checkouts),\n",
    "    'Lab Bookings': len(lab_bookings),\n",
    "    'CCTV Detections': len(cctv_frames)\n",
    "}\n",
    "for event_type, count in sorted(event_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    percentage = (count / total_events) * 100\n",
    "    print(f\"   {event_type:.<25} {count:>8,} ({percentage:>5.1f}%)\")\n",
    "\n",
    "# 3. Most Active Locations (using correct column names)\n",
    "print(f\"\\n Most Active Locations:\")\n",
    "all_locations = pd.concat([\n",
    "    card_swipes['location_id'],      # Card swipes use location_id\n",
    "    wifi_logs['ap_id'],               # WiFi uses ap_id\n",
    "    lab_bookings['room_id'],          # Lab bookings use room_id\n",
    "    cctv_frames['location_id']        # CCTV uses location_id\n",
    "]).value_counts().head(10)\n",
    "\n",
    "for location, count in all_locations.items():\n",
    "    print(f\"   {location:.<25} {count:>8,} events\")\n",
    "\n",
    "# 4. Activity by Hour\n",
    "print(f\"\\n Activity Distribution by Hour:\")\n",
    "card_swipes_copy = card_swipes.copy()\n",
    "card_swipes_copy['hour'] = pd.to_datetime(card_swipes_copy['timestamp'], errors='coerce').dt.hour\n",
    "hourly_activity = card_swipes_copy['hour'].value_counts().sort_index()\n",
    "\n",
    "max_count = hourly_activity.max() if len(hourly_activity) > 0 else 1\n",
    "\n",
    "for hour in range(24):\n",
    "    count = hourly_activity.get(hour, 0)\n",
    "    bar_length = int((count / max_count) * 50) if max_count > 0 else 0\n",
    "    bar = '█' * bar_length\n",
    "    print(f\"   {hour:02d}:00  {bar} {count:>5,}\")\n",
    "\n",
    "print(\"\\n Analytics complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "660f13f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column check:\n",
      "WiFi logs columns: ['device_hash', 'ap_id', 'timestamp', 'entity_id']\n",
      "Lab bookings columns: ['booking_id', 'entity_id', 'room_id', 'start_time', 'end_time', 'attended (YES/NO)']\n"
     ]
    }
   ],
   "source": [
    "# Check actual column names in each dataset\n",
    "print(\"Column check:\")\n",
    "print(\"WiFi logs columns:\", list(wifi_logs.columns))\n",
    "print(\"Lab bookings columns:\", list(lab_bookings.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a97c546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ANOMALY DETECTION - FULL SYSTEM SCAN\n",
      "================================================================================\n",
      "Scanning 50 entities for anomalies...\n",
      "\n",
      " Scanned 50 entities\n",
      " Found 46 entities with anomalies\n",
      "\n",
      " Top 10 Entities with Most Anomalies:\n",
      "\n",
      "1. Neha Mehta (E100000)\n",
      "   Total Anomalies: 2\n",
      "   • [MEDIUM] late_night_activity: 2 activities between 11 PM - 5 AM\n",
      "   • [MEDIUM] long_inactivity: 3 gaps of >48 hours without activity\n",
      "\n",
      "2. Neha Kumar (E100001)\n",
      "   Total Anomalies: 2\n",
      "   • [MEDIUM] late_night_activity: 3 activities between 11 PM - 5 AM\n",
      "   • [MEDIUM] long_inactivity: 2 gaps of >48 hours without activity\n",
      "\n",
      "3. Neha Singh (E100002)\n",
      "   Total Anomalies: 2\n",
      "   • [MEDIUM] late_night_activity: 2 activities between 11 PM - 5 AM\n",
      "   • [MEDIUM] long_inactivity: 2 gaps of >48 hours without activity\n",
      "\n",
      "4. Rohan Desai (E100004)\n",
      "   Total Anomalies: 2\n",
      "   • [MEDIUM] late_night_activity: 3 activities between 11 PM - 5 AM\n",
      "   • [MEDIUM] long_inactivity: 3 gaps of >48 hours without activity\n",
      "\n",
      "5. Neha Rao (E100005)\n",
      "   Total Anomalies: 2\n",
      "   • [MEDIUM] late_night_activity: 1 activities between 11 PM - 5 AM\n",
      "   • [MEDIUM] long_inactivity: 1 gaps of >48 hours without activity\n",
      "\n",
      "6. Aarav Gupta (E100006)\n",
      "   Total Anomalies: 2\n",
      "   • [MEDIUM] late_night_activity: 2 activities between 11 PM - 5 AM\n",
      "   • [MEDIUM] long_inactivity: 3 gaps of >48 hours without activity\n",
      "\n",
      "7. Sana Patel (E100007)\n",
      "   Total Anomalies: 2\n",
      "   • [MEDIUM] late_night_activity: 1 activities between 11 PM - 5 AM\n",
      "   • [MEDIUM] long_inactivity: 2 gaps of >48 hours without activity\n",
      "\n",
      "8. Siddharth Rao (E100008)\n",
      "   Total Anomalies: 2\n",
      "   • [MEDIUM] late_night_activity: 2 activities between 11 PM - 5 AM\n",
      "   • [MEDIUM] long_inactivity: 2 gaps of >48 hours without activity\n",
      "\n",
      "9. Neha Malhotra (E100010)\n",
      "   Total Anomalies: 2\n",
      "   • [MEDIUM] late_night_activity: 2 activities between 11 PM - 5 AM\n",
      "   • [MEDIUM] long_inactivity: 2 gaps of >48 hours without activity\n",
      "\n",
      "10. Karan Gupta (E100014)\n",
      "   Total Anomalies: 2\n",
      "   • [MEDIUM] late_night_activity: 1 activities between 11 PM - 5 AM\n",
      "   • [MEDIUM] long_inactivity: 2 gaps of >48 hours without activity\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== SCAN ALL ENTITIES FOR ANOMALIES =====\n",
    "print(\"\\n ANOMALY DETECTION - FULL SYSTEM SCAN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_anomalies = []\n",
    "\n",
    "# Scan first 50 entities (to save time - you can increase this later)\n",
    "sample_entities = profiles.head(50)\n",
    "\n",
    "print(f\"Scanning {len(sample_entities)} entities for anomalies...\")\n",
    "\n",
    "for idx, entity_row in sample_entities.iterrows():\n",
    "    entity_id = entity_row['entity_id']\n",
    "    entity_name = entity_row['name']\n",
    "    \n",
    "    # Generate timeline\n",
    "    timeline = generate_entity_timeline(entity_id)\n",
    "    \n",
    "    # Detect anomalies\n",
    "    if len(timeline) > 0:\n",
    "        anomalies = detect_anomalies(entity_id, timeline)\n",
    "        \n",
    "        if len(anomalies) > 0:\n",
    "            all_anomalies.append({\n",
    "                'entity_id': entity_id,\n",
    "                'name': entity_name,\n",
    "                'anomaly_count': len(anomalies),\n",
    "                'anomalies': anomalies\n",
    "            })\n",
    "\n",
    "print(f\"\\n Scanned {len(sample_entities)} entities\")\n",
    "print(f\" Found {len(all_anomalies)} entities with anomalies\")\n",
    "\n",
    "# Show top 10 entities with most anomalies\n",
    "print(f\"\\n Top 10 Entities with Most Anomalies:\")\n",
    "sorted_anomalies = sorted(all_anomalies, key=lambda x: x['anomaly_count'], reverse=True)[:10]\n",
    "\n",
    "for i, entity_data in enumerate(sorted_anomalies, 1):\n",
    "    print(f\"\\n{i}. {entity_data['name']} ({entity_data['entity_id']})\")\n",
    "    print(f\"   Total Anomalies: {entity_data['anomaly_count']}\")\n",
    "    for anomaly in entity_data['anomalies']:\n",
    "        print(f\"   • [{anomaly['severity'].upper()}] {anomaly['type']}: {anomaly['details']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c80723e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " EXPORTING RESULTS\n",
      "================================================================================\n",
      " Exported: entity_profiles.csv (7,000 records)\n",
      " Exported: sample_timeline.csv (6 records)\n",
      " Exported: anomaly_report.csv (75 records)\n",
      " Exported: system_statistics.csv\n",
      "\n",
      " All results saved to: c:\\Users\\nitish kumar yadav\\Downloads\\campus project\\notebook\\..\\output\n",
      "\n",
      " CAMPUS SECURITY SYSTEM ANALYSIS COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== EXPORT RESULTS =====\n",
    "print(\"\\n EXPORTING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create output folder\n",
    "import os\n",
    "output_folder = Path('../output')\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 1. Export entity profiles with linked identifiers\n",
    "profiles.to_csv(output_folder / 'entity_profiles.csv', index=False)\n",
    "print(f\" Exported: entity_profiles.csv ({len(profiles):,} records)\")\n",
    "\n",
    "# 2. Export sample timeline\n",
    "sample_timeline = generate_entity_timeline(profiles.iloc[0]['entity_id'])\n",
    "sample_timeline.to_csv(output_folder / 'sample_timeline.csv', index=False)\n",
    "print(f\" Exported: sample_timeline.csv ({len(sample_timeline):,} records)\")\n",
    "\n",
    "# 3. Export anomaly report\n",
    "anomaly_report = []\n",
    "for entity_data in all_anomalies:\n",
    "    for anomaly in entity_data['anomalies']:\n",
    "        anomaly_report.append({\n",
    "            'entity_id': entity_data['entity_id'],\n",
    "            'name': entity_data['name'],\n",
    "            'anomaly_type': anomaly['type'],\n",
    "            'severity': anomaly['severity'],\n",
    "            'details': anomaly['details']\n",
    "        })\n",
    "\n",
    "if len(anomaly_report) > 0:\n",
    "    anomaly_df = pd.DataFrame(anomaly_report)\n",
    "    anomaly_df.to_csv(output_folder / 'anomaly_report.csv', index=False)\n",
    "    print(f\" Exported: anomaly_report.csv ({len(anomaly_df):,} records)\")\n",
    "else:\n",
    "    print(f\" No anomalies to export\")\n",
    "\n",
    "# 4. Export system statistics\n",
    "stats = {\n",
    "    'metric': [\n",
    "        'Total Entities', \n",
    "        'Total Events', \n",
    "        'Entities with Anomalies',\n",
    "        'Total Anomaly Count',\n",
    "        'Scan Coverage'\n",
    "    ],\n",
    "    'value': [\n",
    "        total_entities,\n",
    "        total_events,\n",
    "        len(all_anomalies),\n",
    "        sum(e['anomaly_count'] for e in all_anomalies),\n",
    "        f\"{len(sample_entities)}/{total_entities} entities scanned\"\n",
    "    ]\n",
    "}\n",
    "stats_df = pd.DataFrame(stats)\n",
    "stats_df.to_csv(output_folder / 'system_statistics.csv', index=False)\n",
    "print(f\" Exported: system_statistics.csv\")\n",
    "\n",
    "print(f\"\\n All results saved to: {output_folder.absolute()}\")\n",
    "print(\"\\n CAMPUS SECURITY SYSTEM ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "campus_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
